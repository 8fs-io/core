#!/usr/bin/env python3
"""
8fs RAG Server Test Client

This script demonstrates how to use the 8fs RAG server for:
1. Storing documents with embeddings
2. Searching for relevant context
3. Generating responses using RAG

Usage:
    python3 test_rag_client.py

Prerequisites:
    - 8fs server running on localhost:8080
    - Ollama running on localhost:11434 with all-minilm and llama3.2 models
    - Python 3.6+ with requests library
"""

import json
import requests
import time
from typing import Dict, List, Any

class RAGClient:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def health_check(self) -> bool:
        """Check if the RAG service is healthy"""
        try:
            response = self.session.get(f"{self.base_url}/api/v1/chat/health")
            return response.status_code == 200
        except Exception as e:
            print(f"Health check failed: {e}")
            return False
    
    def store_document(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """Store a document with embeddings"""
        if metadata is None:
            metadata = {}
        
        # Add content to metadata for retrieval
        metadata["content"] = content
        metadata["doc_id"] = doc_id
        metadata["timestamp"] = time.time()
        
        # We'll use a simple embedding (in real usage, this would be generated by AI service)
        # For now, we'll let the server generate the embedding via text search
        payload = {
            "id": doc_id,
            "embedding": [0.1] * 384,  # Placeholder - server will generate real embedding
            "metadata": metadata
        }
        
        try:
            response = self.session.post(f"{self.base_url}/api/v1/vectors/embeddings", json=payload)
            if response.status_code == 200:
                print(f"✅ Stored document: {doc_id}")
                return True
            else:
                print(f"❌ Failed to store {doc_id}: {response.status_code} {response.text}")
                return False
        except Exception as e:
            print(f"❌ Error storing document {doc_id}: {e}")
            return False
    
    def search_context(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """Search for relevant context documents"""
        payload = {
            "query": query,
            "top_k": top_k
        }
        
        try:
            response = self.session.post(f"{self.base_url}/api/v1/chat/search/context", json=payload)
            if response.status_code == 200:
                result = response.json()
                print(f"🔍 Found {len(result.get('documents', []))} relevant documents")
                return result
            else:
                print(f"❌ Context search failed: {response.status_code} {response.text}")
                return {}
        except Exception as e:
            print(f"❌ Error searching context: {e}")
            return {}
    
    def chat_completion(self, query: str, max_tokens: int = 500, temperature: float = 0.7, top_k: int = 5) -> Dict[str, Any]:
        """Perform RAG-based chat completion"""
        payload = {
            "query": query,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_k": top_k
        }
        
        try:
            response = self.session.post(f"{self.base_url}/api/v1/chat/completions", json=payload)
            if response.status_code == 200:
                result = response.json()
                print(f"💬 Chat completion successful")
                return result
            else:
                print(f"❌ Chat completion failed: {response.status_code} {response.text}")
                return {}
        except Exception as e:
            print(f"❌ Error in chat completion: {e}")
            return {}

def main():
    print("🚀 8fs RAG Server Test Client")
    print("=" * 50)
    
    client = RAGClient()
    
    # Check health
    print("\n1. Health Check")
    if not client.health_check():
        print("❌ RAG service is not healthy. Make sure 8fs server is running.")
        return
    print("✅ RAG service is healthy")
    
    # Sample documents about 8fs
    documents = [
        {
            "id": "doc_8fs_overview",
            "content": "8fs is an S3-compatible storage server with native vector search capabilities. It combines traditional object storage with AI-powered semantic search, enabling efficient document indexing and retrieval.",
            "metadata": {"source": "overview.md", "type": "documentation"}
        },
        {
            "id": "doc_8fs_features", 
            "content": "8fs features include S3 API compatibility, vector storage with SQLite-vec, async document indexing, text-based search APIs, and support for multiple AI providers like Ollama, OpenAI, and AWS Bedrock.",
            "metadata": {"source": "features.md", "type": "documentation"}
        },
        {
            "id": "doc_8fs_performance",
            "content": "8fs delivers high performance with 1,700+ vectors per second insert rate and 1.8-8.9 queries per second search performance. It uses SQLite-vec for optimized vector operations with cosine similarity search.",
            "metadata": {"source": "performance.md", "type": "documentation"}
        },
        {
            "id": "doc_8fs_rag",
            "content": "8fs now includes RAG capabilities with /api/v1/chat/completions endpoint for OpenAI-compatible chat completions, context search, and generation with retrieval-augmented responses using embedded documents.",
            "metadata": {"source": "rag.md", "type": "documentation"}
        }
    ]
    
    # Store documents
    print("\n2. Storing Sample Documents")
    for doc in documents:
        client.store_document(doc["id"], doc["content"], doc["metadata"])
    
    # Wait a moment for indexing
    print("\n⏳ Waiting for indexing...")
    time.sleep(2)
    
    # Test context search
    print("\n3. Context Search Test")
    query = "What are the performance characteristics of 8fs?"
    context_result = client.search_context(query, top_k=3)
    
    if context_result.get("documents"):
        print(f"\n📄 Retrieved {len(context_result['documents'])} context documents:")
        for i, doc in enumerate(context_result["documents"][:2], 1):
            print(f"   {i}. {doc.get('source', 'Unknown')}: Score {doc.get('score', 0):.3f}")
            print(f"      {doc.get('content', '')[:100]}...")
    
    # Test RAG chat completion
    print("\n4. RAG Chat Completion Test")
    queries = [
        "What is 8fs and what makes it unique?",
        "How fast is 8fs for vector operations?",
        "What RAG capabilities does 8fs provide?"
    ]
    
    for query in queries:
        print(f"\n❓ Query: {query}")
        response = client.chat_completion(query, max_tokens=300, temperature=0.7, top_k=3)
        
        if response.get("choices"):
            choice = response["choices"][0]
            answer = choice["message"]["content"]
            print(f"🤖 Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}")
            
            if response.get("context"):
                doc_count = len(response["context"].get("documents", []))
                print(f"📚 Used {doc_count} context documents")
            
            if response.get("usage"):
                usage = response["usage"]
                print(f"📊 Tokens: {usage.get('total_tokens', 0)} total ({usage.get('prompt_tokens', 0)} prompt + {usage.get('completion_tokens', 0)} completion)")
        
        print("-" * 50)
    
    print("\n✅ RAG testing completed!")
    print("\nNext steps:")
    print("- Upload your own documents via S3 API or direct vector storage")
    print("- Use /api/v1/chat/completions for production RAG queries")
    print("- Monitor performance with /api/v1/chat/health endpoint")

if __name__ == "__main__":
    main()